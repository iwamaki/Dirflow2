/* =========================================
    LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šãƒ»APIå‘¼ã³å‡ºã—å‡¦ç†
   ========================================= */

import { parseStructuredResponse, validateCommand, getMockResponse } from './response-utils.js';

// LLM Providers Configuration
export const LLM_PROVIDERS = {
    claude: {
        name: 'Claude',
        apiUrl: 'https://api.anthropic.com/v1/messages',
        models: [
            'claude-3-haiku-20240307',
            'claude-3-5-haiku-20241022',
            'claude-sonnet-4-20250514',
            'claude-opus-4-1-20250805'
        ],
        defaultModel: 'claude-3-5-haiku-20241022'
    },
    openai: {
        name: 'OpenAI GPT',
        apiUrl: 'https://api.openai.com/v1/chat/completions',
        models: [
            'gpt-4.1-mini',
            'gpt-4',
            'gpt-4-turbo'
        ],
        defaultModel: 'gpt-4'
    },
    gemini: {
        name: 'Google Gemini',
        apiUrl: 'https://generativelanguage.googleapis.com/v1beta/models',
        models: [
            'gemini-2.5-flash-lite',
            'gemini-2.5-flash',
            'gemini-2.5-pro'
        ],
        defaultModel: 'gemini-2.5-flash'
    },
    local: {
        name: 'Local LLM',
        apiUrl: 'http://localhost:11434/v1/chat/completions',
        models: [
            'llama3',
            'phi3',
            'gemma3',
            'gpt-oss:20b'
        ],
        defaultModel: 'llama3'
    }
};

// ãƒ™ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œæ©Ÿèƒ½ï¼‰
export const BASE_SYSTEM_PROMPT = `ã‚ãªãŸã¯é«˜åº¦ãªAIãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®è‡ªç„¶ãªä¼šè©±ã‚’è¡Œã„ãªãŒã‚‰ã€å¿…è¦ã«å¿œã˜ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ æ“ä½œã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚

å¿œç­”ã¯å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§è¡Œã£ã¦ãã ã•ã„ï¼š

{
  "message": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®è‡ªç„¶ãªå¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆå¿…é ˆï¼‰",
  "commands": [
    {
      "action": "create_file",
      "path": "example.txt",
      "content": "ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹",
      "description": "æ“ä½œã®èª¬æ˜"
    }
  ]
}

åˆ©ç”¨å¯èƒ½ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼š
- create_file: ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ (path, content?, description?)
- create_directory: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ (path, description?)
- delete_file: ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ (path, description?)
- copy_file: ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼ (source, destination, description?)
- move_file: ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•/åå‰å¤‰æ›´ (source, destination, description?)
- read_file: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ (path, description?)
- edit_file: ãƒ•ã‚¡ã‚¤ãƒ«ç·¨é›† (path, content, description?)
- list_files: ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§è¡¨ç¤º (path?, description?)
- batch_delete: ä¸€æ‹¬å‰Šé™¤ (paths[], description?)
- batch_copy: ä¸€æ‹¬ã‚³ãƒ”ãƒ¼ (sources[], destination, description?)
- batch_move: ä¸€æ‹¬ç§»å‹• (sources[], destination, description?)

ç¾åœ¨ã®çŠ¶æ…‹:
- ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {{CURRENT_PATH}}
- ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {{FILE_COUNT}}
- ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§: {{FILE_LIST}}
- ä¼šè©±å±¥æ­´æ•°: {{HISTORY_COUNT}}

åŸºæœ¬ãƒ«ãƒ¼ãƒ«:
1. å¿…ãšmessageãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å«ã‚ã‚‹
2. ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œä¸è¦ãªå ´åˆã¯ commands: [] ã‚’ä½¿ç”¨
3. ãƒ‘ã‚¹ã¯ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã¾ãŸã¯çµ¶å¯¾ãƒ‘ã‚¹
4. å±é™ºãªæ“ä½œï¼ˆå‰Šé™¤ï¼‰ã®å ´åˆã¯ç¢ºèªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å«ã‚ã‚‹
5. ã‚¨ãƒ©ãƒ¼ãŒäºˆæƒ³ã•ã‚Œã‚‹å ´åˆã¯äº‹å‰ã«è­¦å‘Šã™ã‚‹
6. ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯å¿…ãšJSONã¨ã—ã¦æœ‰åŠ¹ãªå½¢å¼ã«ã™ã‚‹
7. ä¼šè©±ãŒé•·ããªã£ãŸå ´åˆï¼ˆ15å›ä»¥ä¸Šã®å¾€å¾©ï¼‰ã¯æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆé–‹å§‹ã‚’ææ¡ˆã™ã‚‹`;

// ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ±åˆã™ã‚‹é–¢æ•°
export function combineSystemPrompts(customPrompt, basePrompt, context = {}) {
    let combinedPrompt = '';
    
    // ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒã‚ã‚‹å ´åˆã¯æœ€åˆã«é…ç½®
    if (customPrompt && customPrompt.content) {
        combinedPrompt += `=== ã‚«ã‚¹ã‚¿ãƒ ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ===\n`;
        combinedPrompt += `ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå: ${customPrompt.name}\n`;
        if (customPrompt.description) {
            combinedPrompt += `èª¬æ˜: ${customPrompt.description}\n`;
        }
        combinedPrompt += `\n${customPrompt.content}\n\n`;
    }
    
    // ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œæ©Ÿèƒ½ï¼‰ã‚’è¿½åŠ 
    combinedPrompt += `=== ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ===\n`;
    combinedPrompt += basePrompt;
    
    // ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒã‚ã‚‹å ´åˆã®çµ±åˆæŒ‡ç¤º
    if (customPrompt && customPrompt.content) {
        combinedPrompt += `\n\n=== çµ±åˆæŒ‡ç¤º ===\n`;
        combinedPrompt += `ä¸Šè¨˜ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ€§æ ¼ãƒ»å½¹å‰²ãƒ»æŒ‡ç¤ºã«å¾“ã„ã¤ã¤ã€ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œãŒå¿…è¦ãªå ´åˆã¯å¿…ãšJSONå½¢å¼ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚\n`;
        combinedPrompt += `ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸¡æ–¹ã®è¦æ±‚ã‚’æº€ãŸã™ã‚ˆã†å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚`;
    }
    
    // ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã®ç½®æ›
    const fileList = context.fileList || [];
    const conversationHistory = context.conversationHistory || [];
    
    combinedPrompt = combinedPrompt
        .replace('{{CURRENT_PATH}}', context.currentPath || '/workspace')
        .replace('{{FILE_COUNT}}', fileList.length)
        .replace('{{FILE_LIST}}', JSON.stringify(fileList, null, 2))
        .replace('{{HISTORY_COUNT}}', conversationHistory.length);

    return combinedPrompt;
}

// Claude API Callï¼ˆã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯¾å¿œï¼‰
export async function callClaudeAPI(message, model = 'claude-3-sonnet-20240229', context = {}) {
    const fileList = context.fileList || [];
    const conversationHistory = context.conversationHistory || [];
    const customPrompt = context.customPrompt;
    
    // ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ±åˆ
    const systemPrompt = combineSystemPrompts(customPrompt, BASE_SYSTEM_PROMPT, context);

    // ä¼šè©±å±¥æ­´ã‚’Claudeå½¢å¼ã«å¤‰æ›
    const messages = [];
    
    // éå»ã®ä¼šè©±å±¥æ­´ã‚’è¿½åŠ ï¼ˆæœ€æ–°10ä»¶ã®ã¿ï¼‰
    const recentHistory = conversationHistory.slice(-10);
    for (const exchange of recentHistory) {
        messages.push({ role: 'user', content: exchange.user });
        if (exchange.ai) {
            messages.push({ role: 'assistant', content: exchange.ai });
        }
    }
    
    // ç¾åœ¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    let contextInfo = `[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±]\nç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: ${context.currentPath || '/workspace'}\nãƒ•ã‚¡ã‚¤ãƒ«æ•°: ${fileList.length}\nç¾åœ¨ç·¨é›†ä¸­: ${context.currentFile || 'ãªã—'}`;
    
    // ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æƒ…å ±ã‚’è¿½åŠ 
    if (customPrompt && customPrompt.enabled) {
        contextInfo += `\nã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ${customPrompt.name} (æœ‰åŠ¹)`;
    }
    
    // ç¾åœ¨é–‹ã„ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’è¿½åŠ 
    if (context.openFileInfo) {
        contextInfo += `\n\n[ç¾åœ¨é–‹ã„ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°]\n${context.openFileInfo}`;
    }
    
    messages.push({
        role: 'user',
        content: `${message}\n\n${contextInfo}`
    });

    if (process.env.DEBUG_LLM_CONTEXT === 'true') {
        console.log('--- Claude API Request ---');
        console.log('Messages:', JSON.stringify(messages, null, 2));
        console.log('--------------------------');
    }

    const response = await fetch('https://api.anthropic.com/v1/messages', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'x-api-key': process.env.ANTHROPIC_API_KEY,
            'anthropic-version': '2023-06-01'
        },
        body: JSON.stringify({
            model: model,
            max_tokens: 2048,
            messages: messages,
            system: systemPrompt
        })
    });

    if (!response.ok) {
        throw new Error(`Claude API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    return data.content[0].text;
}

// Gemini API Callï¼ˆã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯¾å¿œï¼‰
export async function callGeminiAPI(message, model = 'gemini-1.5-pro', context = {}) {
    const fileList = context.fileList || [];
    const conversationHistory = context.conversationHistory || [];
    const customPrompt = context.customPrompt;
    
    // ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ±åˆ
    const systemPrompt = combineSystemPrompts(customPrompt, BASE_SYSTEM_PROMPT, context);

    // ä¼šè©±å±¥æ­´ã‚’å«ã‚€å†…å®¹ã‚’æ§‹ç¯‰
    let fullContent = systemPrompt + '\n\n';
    
    // éå»ã®ä¼šè©±å±¥æ­´ã‚’è¿½åŠ ï¼ˆæœ€æ–°10ä»¶ã®ã¿ï¼‰
    const recentHistory = conversationHistory.slice(-10);
    if (recentHistory.length > 0) {
        fullContent += '=== éå»ã®ä¼šè©±å±¥æ­´ ===\n';
        for (const exchange of recentHistory) {
            fullContent += `ãƒ¦ãƒ¼ã‚¶ãƒ¼: ${exchange.user}\n`;
            if (exchange.ai) {
                fullContent += `AI: ${exchange.ai}\n`;
            }
            fullContent += '\n';
        }
        fullContent += '=== ç¾åœ¨ã®è³ªå• ===\n';
    }
    
    let contextInfo = `[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±]\nç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: ${context.currentPath || '/workspace'}\nãƒ•ã‚¡ã‚¤ãƒ«æ•°: ${fileList.length}`;
    
    // ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æƒ…å ±ã‚’è¿½åŠ 
    if (customPrompt && customPrompt.enabled) {
        contextInfo += `\nã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ${customPrompt.name} (æœ‰åŠ¹)`;
    }
    
    // ç¾åœ¨é–‹ã„ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’è¿½åŠ 
    if (context.openFileInfo) {
        contextInfo += `\n\n[ç¾åœ¨é–‹ã„ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°]\n${context.openFileInfo}`;
    }
    
    fullContent += `ãƒ¦ãƒ¼ã‚¶ãƒ¼: ${message}\n\n${contextInfo}`;

    if (process.env.DEBUG_LLM_CONTEXT === 'true') {
        console.log('--- Gemini API Request ---');
        console.log('Full Content:', fullContent);
        console.log('--------------------------');
    }

    const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${process.env.GOOGLE_API_KEY}`, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            contents: [
                {
                    parts: [
                        {
                            text: fullContent
                        }
                    ]
                }
            ],
            generationConfig: {
                temperature: 0.7,
                topK: 40,
                topP: 0.95,
                maxOutputTokens: 2048,
                stopSequences: []
            },
            safetySettings: [
                {
                    category: "HARM_CATEGORY_HARASSMENT",
                    threshold: "BLOCK_MEDIUM_AND_ABOVE"
                },
                {
                    category: "HARM_CATEGORY_HATE_SPEECH",
                    threshold: "BLOCK_MEDIUM_AND_ABOVE"
                },
                {
                    category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                    threshold: "BLOCK_MEDIUM_AND_ABOVE"
                },
                {
                    category: "HARM_CATEGORY_DANGEROUS_CONTENT",
                    threshold: "BLOCK_MEDIUM_AND_ABOVE"
                }
            ]
        })
    });

    if (!response.ok) {
        throw new Error(`Gemini API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    
    if (data.candidates && data.candidates[0] && data.candidates[0].content) {
        return data.candidates[0].content.parts[0].text;
    } else {
        throw new Error('Unexpected Gemini API response format');
    }
}

// OpenAI API Callï¼ˆã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯¾å¿œï¼‰
export async function callOpenAIAPI(message, model = 'gpt-4', context = {}) {
    const fileList = context.fileList || [];
    const conversationHistory = context.conversationHistory || [];
    const customPrompt = context.customPrompt;
    
    // ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ±åˆ
    const systemPrompt = combineSystemPrompts(customPrompt, BASE_SYSTEM_PROMPT, context);

    const messages = [
        {
            role: 'system',
            content: systemPrompt
        }
    ];
    
    // éå»ã®ä¼šè©±å±¥æ­´ã‚’è¿½åŠ ï¼ˆæœ€æ–°10ä»¶ã®ã¿ï¼‰
    const recentHistory = conversationHistory.slice(-10);
    for (const exchange of recentHistory) {
        messages.push({ role: 'user', content: exchange.user });
        if (exchange.ai) {
            messages.push({ role: 'assistant', content: exchange.ai });
        }
    }
    
    // ç¾åœ¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    let contextInfo = `[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±]\nç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: ${context.currentPath || '/workspace'}\nãƒ•ã‚¡ã‚¤ãƒ«æ•°: ${fileList.length}`;
    
    // ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æƒ…å ±ã‚’è¿½åŠ 
    if (customPrompt && customPrompt.enabled) {
        contextInfo += `\nã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ${customPrompt.name} (æœ‰åŠ¹)`;
    }
    
    // ç¾åœ¨é–‹ã„ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’è¿½åŠ 
    if (context.openFileInfo) {
        contextInfo += `\n\n[ç¾åœ¨é–‹ã„ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°]\n${context.openFileInfo}`;
    }
    
    messages.push({
        role: 'user',
        content: `${message}\n\n${contextInfo}`
    });

    if (process.env.DEBUG_LLM_CONTEXT === 'true') {
        console.log('--- OpenAI API Request ---');
        console.log('Messages:', JSON.stringify(messages, null, 2));
        console.log('--------------------------');
    }

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`
        },
        body: JSON.stringify({
            model: model,
            max_tokens: 2048,
            messages: messages,
            temperature: 0.7
        })
    });

    if (!response.ok) {
        throw new Error(`OpenAI API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    return data.choices[0].message.content;
}

// Local LLM API Call (OpenAI-compatible)
export async function callLocalLLMAPI(message, model, context = {}) {
    const fileList = context.fileList || [];
    const conversationHistory = context.conversationHistory || [];
    const customPrompt = context.customPrompt;
    
    const systemPrompt = combineSystemPrompts(customPrompt, BASE_SYSTEM_PROMPT, context);

    const messages = [
        {
            role: 'system',
            content: systemPrompt
        }
    ];
    
    const recentHistory = conversationHistory.slice(-10);
    for (const exchange of recentHistory) {
        messages.push({ role: 'user', content: exchange.user });
        if (exchange.ai) {
            messages.push({ role: 'assistant', content: exchange.ai });
        }
    }
    
    let contextInfo = `[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±]\nç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: ${context.currentPath || '/workspace'}\nãƒ•ã‚¡ã‚¤ãƒ«æ•°: ${fileList.length}`;
    
    if (customPrompt && customPrompt.enabled) {
        contextInfo += `\nã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ${customPrompt.name} (æœ‰åŠ¹)`;
    }
    
    if (context.openFileInfo) {
        contextInfo += `\n\n[ç¾åœ¨é–‹ã„ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°]\n${context.openFileInfo}`;
    }
    
    messages.push({
        role: 'user',
        content: `${message}\n\n${contextInfo}`
    });

    if (process.env.DEBUG_LLM_CONTEXT === 'true') {
        console.log('--- Local LLM API Request ---');
        console.log(`History Length: ${conversationHistory.length} `);
        console.log('Messages:', JSON.stringify(messages, null, 2));
        console.log('--------------------------');
    }

    const response = await fetch(LLM_PROVIDERS.local.apiUrl, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            model: model,
            max_tokens: 2048,
            messages: messages,
            temperature: 0.7
        })
    });

    if (!response.ok) {
        throw new Error(`Local LLM API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    return data.choices[0].message.content;
}

// ãƒ¡ã‚¤ãƒ³ã®ãƒãƒ£ãƒƒãƒˆå‡¦ç†é–¢æ•°ï¼ˆã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹ï¼‰
export async function handleChatRequest(message, provider = 'claude', model, context = {}) {
    // ä¼šè©±å±¥æ­´åˆ¶é™ãƒã‚§ãƒƒã‚¯
    const conversationHistory = context.conversationHistory || [];
    let shouldSuggestNewChat = false;
    
    if (conversationHistory.length >= 15) {
        shouldSuggestNewChat = true;
    }

    // ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    if (context.customPrompt && context.customPrompt.enabled) {
        console.log(`ğŸ“ Using custom prompt: "${context.customPrompt.name}" (${context.customPrompt.id})`);
    }

    let response;
    const selectedModel = model || LLM_PROVIDERS[provider]?.defaultModel;

    switch (provider) {
        case 'claude':
            if (!process.env.ANTHROPIC_API_KEY) {
                throw new Error('Anthropic API key not configured');
            }
            response = await callClaudeAPI(message, selectedModel, context);
            break;

        case 'openai':
            if (!process.env.OPENAI_API_KEY) {
                throw new Error('OpenAI API key not configured');
            }
            response = await callOpenAIAPI(message, selectedModel, context);
            break;

        case 'gemini':
            if (!process.env.GOOGLE_API_KEY) {
                throw new Error('Google API key not configured');
            }
            response = await callGeminiAPI(message, selectedModel, context);
            break;

        case 'local':
            response = await callLocalLLMAPI(message, selectedModel, context);
            break;

        default:
            console.warn(`Unknown provider: ${provider}, using mock response`);
            const mockResult = getMockResponse(message, context);
            return {
                message: mockResult.message,
                commands: mockResult.commands,
                provider: 'fallback',
                model: 'mock',
                timestamp: new Date().toISOString(),
                warning: 'ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”: æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒç„¡åŠ¹ã§ã™',
                shouldSuggestNewChat: shouldSuggestNewChat,
                customPromptUsed: false
            };
    }

    // æ§‹é€ åŒ–å¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹
    const parsedResponse = parseStructuredResponse(response);

    // ã‚³ãƒãƒ³ãƒ‰ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    const validatedCommands = [];
    if (parsedResponse.commands && parsedResponse.commands.length > 0) {
        for (const command of parsedResponse.commands) {
            try {
                validateCommand(command);
                validatedCommands.push(command);
            } catch (error) {
                console.warn(`Invalid command filtered out: ${error.message}`);
                // ç„¡åŠ¹ãªã‚³ãƒãƒ³ãƒ‰ã¯é™¤å¤–ã™ã‚‹ãŒã€ã‚¨ãƒ©ãƒ¼ã«ã¯ã—ãªã„
            }
        }
    }

    // æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆææ¡ˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    let finalMessage = parsedResponse.message;
    if (shouldSuggestNewChat && !finalMessage.includes('æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ')) {
        finalMessage += '\n\nğŸ’¡ **ãƒ’ãƒ³ãƒˆ**: ä¼šè©±ãŒé•·ããªã£ã¦ãã¾ã—ãŸã€‚ã‚ˆã‚Šè‰¯ã„å¿œç­”ã®ãŸã‚ã€æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã‚’é–‹å§‹ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ï¼';
    }

    // ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½¿ç”¨ã®æƒ…å ±
    const customPromptUsed = !!(context.customPrompt && context.customPrompt.enabled);

    return { 
        message: finalMessage,
        commands: validatedCommands,
        rawResponse: response,
        provider,
        model: selectedModel,
        timestamp: new Date().toISOString(),
        parseSuccess: parsedResponse.success,
        warning: parsedResponse.warning,
        shouldSuggestNewChat: shouldSuggestNewChat,
        historyCount: conversationHistory.length,
        customPromptUsed: customPromptUsed,
        customPromptName: customPromptUsed ? context.customPrompt.name : null
    };
}