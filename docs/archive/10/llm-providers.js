/* =========================================
    LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šãƒ»APIå‘¼ã³å‡ºã—å‡¦ç†
   ========================================= */

/*

## æ¦‚è¦
å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆClaude, OpenAI, Gemini, Local LLMï¼‰ã®è¨­å®šã€APIå‘¼ã³å‡ºã—ã€ãŠã‚ˆã³å¿œç­”ã®è§£æã‚’ä¸€å…ƒçš„ã«ç®¡ç†ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚

## ä¸»è¦æ©Ÿèƒ½
- **å®šæ•°**: LLM_PROVIDERS: å„LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®APIæƒ…å ±ã€åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ã€‚
- **é–¢æ•°**: callClaudeAPI(message, model, context): Claude APIã‚’å‘¼ã³å‡ºã—ã€å¿œç­”ã‚’å‡¦ç†ã€‚
- **é–¢æ•°**: callOpenAIAPI(message, model, context): OpenAI APIã‚’å‘¼ã³å‡ºã—ã€å¿œç­”ã‚’å‡¦ç†ã€‚
- **é–¢æ•°**: callGeminiAPI(message, model, context): Gemini APIã‚’å‘¼ã³å‡ºã—ã€å¿œç­”ã‚’å‡¦ç†ã€‚
- **é–¢æ•°**: callLocalLLMAPI(message, model, context): ãƒ­ãƒ¼ã‚«ãƒ«LLM APIã‚’å‘¼ã³å‡ºã—ã€å¿œç­”ã‚’å‡¦ç†ã€‚
- **é–¢æ•°**: parseStructuredResponse(response): LLMã‹ã‚‰ã®æ§‹é€ åŒ–ã•ã‚ŒãŸå¿œç­”ï¼ˆJSONå½¢å¼ï¼‰ã‚’è§£æã—ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã‚³ãƒãƒ³ãƒ‰ã‚’æŠ½å‡ºã€‚
- **é–¢æ•°**: handleChatRequest(message, provider, model, context): ãƒ¡ã‚¤ãƒ³ã®ãƒãƒ£ãƒƒãƒˆå‡¦ç†é–¢æ•°ã€‚é¸æŠã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®APIã‚’å‘¼ã³å‡ºã—ã€å¿œç­”ã‚’è§£æã—ã¦è¿”å´ã€‚

## ä¾å­˜é–¢ä¿‚
- **ã‚¤ãƒ³ãƒãƒ¼ãƒˆ**:
  - `createPromptBuilder`, `logPromptDebugInfo` (from './prompt-builder.js'): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ã¨ãƒ‡ãƒãƒƒã‚°æƒ…å ±ãƒ­ã‚®ãƒ³ã‚°ã«ä½¿ç”¨ã€‚
- **ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ**: LLM_PROVIDERS, callClaudeAPI, callOpenAIAPI, callGeminiAPI, callLocalLLMAPI, parseStructuredResponse, handleChatRequest

## ç‰¹è¨˜äº‹é …
- è¤‡æ•°ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¯¾å¿œã—ã€æŸ”è»Ÿãªåˆ‡ã‚Šæ›¿ãˆãŒå¯èƒ½ã€‚
- ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€ã“ã¨ã§ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚’ç¢ºä¿ã€‚
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ãƒ­ã‚¸ãƒƒã‚¯ã‚’`prompt-builder.js`ã«åˆ†é›¢ã—ã€å†åˆ©ç”¨æ€§ã‚’é«˜ã‚ã¦ã„ã‚‹ã€‚
- LLMã‹ã‚‰ã®å¿œç­”ã«JSONå½¢å¼ã®ã‚³ãƒãƒ³ãƒ‰ãŒå«ã¾ã‚Œã‚‹å ´åˆã€ãã‚Œã‚’è§£æã—ã¦å®Ÿè¡Œå¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰ã¨ã—ã¦æŠ½å‡ºã€‚
- ä¼šè©±å±¥æ­´ãŒé•·ããªã£ãŸå ´åˆã«æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã‚’ææ¡ˆã™ã‚‹æ©Ÿèƒ½ã€‚
- ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½¿ç”¨çŠ¶æ³ã‚’ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ã€‚
- å„APIå‘¼ã³å‡ºã—ã«ãŠã‘ã‚‹ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ã€æ§‹é€ åŒ–å¿œç­”ã®ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼å‡¦ç†ã€‚

*/

import { createPromptBuilder, logPromptDebugInfo } from './prompt-builder.js';
import { AgentRouter } from './agent-router.js';

// LLM Providers Configuration
export const LLM_PROVIDERS = {
    claude: {
        name: 'Claude',
        apiUrl: 'https://api.anthropic.com/v1/messages',
        models: [
            'claude-3-haiku-20240307',
            'claude-3-5-haiku-20241022',
            'claude-sonnet-4-20250514',
            'claude-opus-4-1-20250805'
        ],
        defaultModel: 'claude-3-5-haiku-20241022'
    },
    openai: {
        name: 'OpenAI GPT',
        apiUrl: 'https://api.openai.com/v1/chat/completions',
        models: [
            'gpt-4.1-mini',
            'gpt-4',
            'gpt-4-turbo'
        ],
        defaultModel: 'gpt-4'
    },
    gemini: {
        name: 'Google Gemini',
        apiUrl: 'https://generativelanguage.googleapis.com/v1beta/models',
        models: [
            'gemini-2.5-flash-lite',
            'gemini-2.5-flash',
            'gemini-2.5-pro'
        ],
        defaultModel: 'gemini-2.5-flash'
    },
    local: {
        name: 'Local LLM',
        apiUrl: 'http://localhost:11434/api/chat',
        models: [
            'phi3:latest',
            'llama3:latest',
            'gemma3:4b',
            'gpt-oss:20b'
        ],
        defaultModel: 'phi3:latest'
    }
};

// Claude API Call
export async function callClaudeAPI(message, model = 'claude-3-5-haiku-20241022', context = {}) {
    const promptBuilder = createPromptBuilder()
        .setCustomPrompt(context.customPrompt)
        .setContext(context)
        .setConversationHistory(context.conversationHistory)
        .setUserMessage(message);
    
    const promptData = promptBuilder.buildForProvider('claude');
    logPromptDebugInfo(promptData, 'claude');

    const response = await fetch('https://api.anthropic.com/v1/messages', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'x-api-key': process.env.ANTHROPIC_API_KEY,
            'anthropic-version': '2023-06-01'
        },
        body: JSON.stringify({
            model: model,
            max_tokens: 2048,
            messages: promptData.messages,
            system: promptData.system
        })
    });

    if (!response.ok) {
        throw new Error(`Claude API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    return data.content[0].text;
}

// OpenAI API Call
export async function callOpenAIAPI(message, model = 'gpt-4', context = {}) {
    const promptBuilder = createPromptBuilder()
        .setCustomPrompt(context.customPrompt)
        .setContext(context)
        .setConversationHistory(context.conversationHistory)
        .setUserMessage(message);
    
    const promptData = promptBuilder.buildForProvider('openai');
    logPromptDebugInfo(promptData, 'openai');

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`
        },
        body: JSON.stringify({
            model: model,
            max_tokens: 2048,
            messages: promptData.messages,
            temperature: 0.7
        })
    });

    if (!response.ok) {
        throw new Error(`OpenAI API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    return data.choices[0].message.content;
}

// Gemini API Call
export async function callGeminiAPI(message, model = 'gemini-2.5-flash', context = {}) {
    const promptBuilder = createPromptBuilder()
        .setCustomPrompt(context.customPrompt)
        .setContext(context)
        .setConversationHistory(context.conversationHistory)
        .setUserMessage(message);
    
    const promptData = promptBuilder.buildForProvider('gemini');
    logPromptDebugInfo(promptData, 'gemini');

    const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${process.env.GOOGLE_API_KEY}`, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            contents: [
                {
                    parts: [
                        {
                            text: promptData.content
                        }
                    ]
                }
            ],
            generationConfig: {
                temperature: 0.7,
                topK: 40,
                topP: 0.95,
                maxOutputTokens: 2048,
                stopSequences: []
            },
            safetySettings: [
                {
                    category: "HARM_CATEGORY_HARASSMENT",
                    threshold: "BLOCK_MEDIUM_AND_ABOVE"
                },
                {
                    category: "HARM_CATEGORY_HATE_SPEECH",
                    threshold: "BLOCK_MEDIUM_AND_ABOVE"
                },
                {
                    category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                    threshold: "BLOCK_MEDIUM_AND_ABOVE"
                },
                {
                    category: "HARM_CATEGORY_DANGEROUS_CONTENT",
                    threshold: "BLOCK_MEDIUM_AND_ABOVE"
                }
            ]
        })
    });

    if (!response.ok) {
        throw new Error(`Gemini API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    
    if (data.candidates && data.candidates[0] && data.candidates[0].content) {
        return data.candidates[0].content.parts[0].text;
    } else {
        throw new Error('Unexpected Gemini API response format');
    }
}

// Local LLM API Call
export async function callLocalLLMAPI(message, model = 'phi3:latest', context = {}) {
    console.log(`[DEBUG] Local LLM API call with model: ${model}`);

    const promptBuilder = createPromptBuilder()
        .setCustomPrompt(context.customPrompt)
        .setContext(context)
        .setConversationHistory(context.conversationHistory)
        .setUserMessage(message);

    const promptData = promptBuilder.buildForProvider('local');
    logPromptDebugInfo(promptData, 'local');

    const response = await fetch(LLM_PROVIDERS.local.apiUrl, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            model: model,
            messages: promptData.messages,
            stream: false
        })
    });

    if (!response.ok) {
        throw new Error(`Local LLM API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    return data.message.content;
}

// ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ã‚³ãƒãƒ³ãƒ‰æ§‹é€ ã‚’è§£æã™ã‚‹é–¢æ•°
export function parseStructuredResponse(response) {
    try {
        const jsonMatch = response.match(/\{[\s\S]*\}/);
        if (!jsonMatch) {
            return { 
                success: false,
                message: response,
                commands: [],
                warning: "JSONãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
            };
        }
        
        const jsonString = jsonMatch[0];
        const data = JSON.parse(jsonString);
        
        return {
            success: true,
            message: data.message || response,
            commands: data.commands || [],
            warning: null
        };
    } catch (error) {
        return {
            success: false,
            message: response,
            commands: [],
            warning: `JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: ${error.message}`
        };
    }
}

// ãƒ¡ã‚¤ãƒ³ã®ãƒãƒ£ãƒƒãƒˆå‡¦ç†é–¢æ•°ï¼ˆã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹ï¼‰
export async function handleChatRequest(message, provider = 'claude', model, context = {}) {
    // å…¥åŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if (!message || typeof message !== 'string') {
        throw new Error('ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç„¡åŠ¹ã§ã™');
    }

    if (!LLM_PROVIDERS[provider]) {
        throw new Error(`ä¸æ˜ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: ${provider}`);
    }

    // ä¼šè©±å±¥æ­´ã®é•·ã•ãƒã‚§ãƒƒã‚¯
    const conversationHistory = context.conversationHistory || [];
    let shouldSuggestNewChat = false;
    if (conversationHistory.length >= 15) {
        shouldSuggestNewChat = true;
    }

    // ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    if (context.customPrompt && context.customPrompt.enabled && process.env.DEBUG_LLM_CONTEXT === 'true') {
        console.log(`ä½¿ç”¨ä¸­ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ${context.customPrompt.name}`);
    }

    const selectedModel = model || LLM_PROVIDERS[provider]?.defaultModel;

    try {
        // ğŸš€ ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
        console.log('ğŸ¤– Using Multi-Agent Routing System');
        const result = await AgentRouter.routeToAgent(message, context, provider, selectedModel);

        // æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆææ¡ˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        let finalMessage = result.message;
        if (shouldSuggestNewChat && !finalMessage.includes('æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ')) {
            finalMessage += '\n\n(æ³¨: ä¼šè©±ãŒé•·ããªã£ã¦ãã¾ã—ãŸã€‚ã‚‚ã—æ–°ã—ã„è©±é¡Œã§ã‚ã‚Œã°ã€æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã®é–‹å§‹ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚)';
        }

        // ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æƒ…å ±ã‚’ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«å«ã‚ã‚‹
        const routingInfo = result.routing || {};
        if (routingInfo.selectedAgent) {
            console.log(`âœ… Task completed by: ${routingInfo.selectedAgent} (${result.agentName || 'Unknown'})`);
        }

        // ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½¿ç”¨ã®æƒ…å ±
        const customPromptUsed = !!(context.customPrompt && context.customPrompt.enabled);

        return {
            message: finalMessage,
            commands: result.commands || [],
            rawResponse: result.rawResponse,
            provider,
            model: selectedModel,
            timestamp: new Date().toISOString(),
            parseSuccess: result.parseSuccess !== false,
            warning: result.warning,
            shouldSuggestNewChat: shouldSuggestNewChat,
            historyCount: conversationHistory.length,
            customPromptUsed: customPromptUsed,
            customPromptName: customPromptUsed ? context.customPrompt.name : null,
            // ğŸ†• ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæƒ…å ±
            agentUsed: result.agentUsed,
            agentName: result.agentName,
            routing: routingInfo
        };

    } catch (error) {
        console.error('âŒ Multi-Agent System Error:', error);

        // ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¾“æ¥ã®ã‚·ãƒ³ã‚°ãƒ«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ–¹å¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        console.log('âš ï¸ Falling back to single-agent mode...');

        let response;
        try {
            switch (provider) {
                case 'claude':
                    response = await callClaudeAPI(message, selectedModel, context);
                    break;
                case 'openai':
                    response = await callOpenAIAPI(message, selectedModel, context);
                    break;
                case 'gemini':
                    response = await callGeminiAPI(message, selectedModel, context);
                    break;
                case 'local':
                    response = await callLocalLLMAPI(message, selectedModel, context);
                    break;
                default:
                    throw new Error(`Unknown provider: ${provider}`);
            }
        } catch (fallbackError) {
            return {
                message: `APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: ${fallbackError.message}`,
                commands: [],
                rawResponse: null,
                provider,
                model: selectedModel,
                timestamp: new Date().toISOString(),
                parseSuccess: false,
                warning: fallbackError.message,
                error: true
            };
        }

        // æ§‹é€ åŒ–å¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹
        const parsedResponse = parseStructuredResponse(response);

        // ã‚³ãƒãƒ³ãƒ‰ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        const validatedCommands = [];
        if (parsedResponse.commands && parsedResponse.commands.length > 0) {
            for (const cmd of parsedResponse.commands) {
                // åŸºæœ¬çš„ãªãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                if (!cmd.action) continue;

                // ãƒ‘ã‚¹ã®æ­£è¦åŒ–ã¨æ¤œè¨¼
                if (cmd.path && typeof cmd.path === 'string') {
                    // ãƒ‘ã‚¹æ­£è¦åŒ–ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã¯ç’°å¢ƒã«ä¾å­˜ï¼‰
                    // ä¾‹: ãƒ‘ã‚¹ã®å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ãªã©
                }

                validatedCommands.push(cmd);
            }
        }

        // æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆææ¡ˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        let finalMessage = parsedResponse.message;
        if (shouldSuggestNewChat && !finalMessage.includes('æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ')) {
            finalMessage += '\n\n(æ³¨: ä¼šè©±ãŒé•·ããªã£ã¦ãã¾ã—ãŸã€‚ã‚‚ã—æ–°ã—ã„è©±é¡Œã§ã‚ã‚Œã°ã€æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã®é–‹å§‹ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚)';
        }

        // ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½¿ç”¨ã®æƒ…å ±
        const customPromptUsed = !!(context.customPrompt && context.customPrompt.enabled);

        return {
            message: finalMessage,
            commands: validatedCommands,
            rawResponse: response,
            provider,
            model: selectedModel,
            timestamp: new Date().toISOString(),
            parseSuccess: parsedResponse.success,
            warning: parsedResponse.warning,
            shouldSuggestNewChat: shouldSuggestNewChat,
            historyCount: conversationHistory.length,
            customPromptUsed: customPromptUsed,
            customPromptName: customPromptUsed ? context.customPrompt.name : null,
            fallbackMode: true
        };
    }
}